{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62-AWzpGUYK4"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzRXXBNsUg1m"
   },
   "outputs": [],
   "source": [
    "#installing necessary packages\n",
    "!pip install keras-tuner --upgrade\n",
    "!pip install np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1YCoDlewqgV"
   },
   "outputs": [],
   "source": [
    "# Making Import modules necessary for hyperparameter tuning and creating CNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hypermodel import HyperModel\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import tensorflow as tf\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6aIqxJCwtbT"
   },
   "outputs": [],
   "source": [
    "#Making additional imports\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing  \n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.random_projection import GaussianRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au2tC4KCUvkb"
   },
   "source": [
    "# Data work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aqn8tlL2UsC-"
   },
   "outputs": [],
   "source": [
    "#load datasets\n",
    "inputData = pd.read_csv(\"GSE59739_DataTable.txt\",sep=\"\\t\") \n",
    "dataNo=1\n",
    "\n",
    "\"\"\"\n",
    "In this programm three scRNA-seq datasets are tested. The datasets should be supplied to the program one by one\n",
    "Comment the tested dataset and uncomment the following datasets with their corresponding number (dataNo)\n",
    "\"\"\"\n",
    "\n",
    "#inputData = pd.read_csv(\"GSE103334_FPKM_CKP25_TOPHAT.txt\",sep=\"\\t\")\n",
    "#dataNo=2 \n",
    "#inputData = pd.read_csv(\"GSE86469_GEO.islet.single.cell.processed.data.RSEM.raw.expected.counts.csv\") \n",
    "#dataNo=3 \n",
    "\n",
    "inputData.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeKgWbxJwwh6"
   },
   "outputs": [],
   "source": [
    "Data=inputData.T\n",
    "new_header1 = Data.iloc[0] \n",
    "Data = Data[1:]\n",
    "Data.columns = new_header1\n",
    "print(Data.shape)\n",
    "Data.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0O00ee_xPdF"
   },
   "outputs": [],
   "source": [
    "#Split the data into train and test data -> train:test =80:20\n",
    "train, test= sklearn.model_selection.train_test_split(Data, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPJE6A1Fbw3s"
   },
   "source": [
    "# Labeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpoptJPCU85s"
   },
   "outputs": [],
   "source": [
    "def yData(data, dataNo):\n",
    "    \n",
    "    \"\"\"\n",
    "    Adds labels to the input data based on the information given in the GEO database\n",
    "    \"\"\"\n",
    "    if dataNo==1:\n",
    "        i1=data.index.shape[0]\n",
    "        Y=[]\n",
    "        row=0\n",
    "        while row<i1:\n",
    "            if data.index.str.contains('128')[row] or data.index.str.contains('129')[row] or data.index.str.contains('130')[row]:\n",
    "                Y.append(0)\n",
    "            elif data.index.str.contains('141')[row] or  data.index.str.contains('142')[row]:\n",
    "                Y.append(1)\n",
    "            elif data.index.str.contains('226')[row] or data.index.str.contains('227')[row] or data.index.str.contains('228')[row] or data.index.str.contains('281')[row] or data.index.str.contains('282')[row]:\n",
    "                Y.append(2)\n",
    "            row=row+1\n",
    "        y=np.array(Y)   \n",
    "        y=y.reshape(i1,1)\n",
    "    elif dataNo==2:\n",
    "        i1=data.index.shape[0]\n",
    "        Y=[]\n",
    "        row=0\n",
    "        while row<i1:\n",
    "            if data.index.str.contains('1w')[row]:\n",
    "                Y.append(1)\n",
    "            elif data.index.str.contains('0w')[row]:\n",
    "                Y.append(0)\n",
    "            elif data.index.str.contains('2w')[row]:\n",
    "                Y.append(2)\n",
    "            elif data.index.str.contains('6w')[row]:\n",
    "                Y.append(3)\n",
    "            row=row+1\n",
    "        y=np.array(Y)   \n",
    "        y=y.reshape(i1,1)\n",
    "    elif dataNo==3:\n",
    "        i1=data.index.shape[0]\n",
    "        Y=[]\n",
    "        row=0\n",
    "        while row<i1:\n",
    "            if data.index.str.contains('10th')[row] or data.index.str.contains('11th')[row] or data.index.str.contains('12th')[row] or data.index.str.contains('13th')[row]:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            row=row+1\n",
    "        y=np.array(Y)   \n",
    "        y=y.reshape(i1,1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoZLPv9rVG7V"
   },
   "outputs": [],
   "source": [
    "print(train)\n",
    "trainY=yData(train, dataNo)\n",
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7CjsvzTVJeQ"
   },
   "outputs": [],
   "source": [
    "print(test)\n",
    "testY=yData(test, dataNo)\n",
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG4OwNDVXKIc"
   },
   "outputs": [],
   "source": [
    "#Random Projection\n",
    "def RandomProjection(test, train, dataNo, n_components):\n",
    "    \"\"\"\n",
    "    Performs the dimensionality reduction using Random Projection method\n",
    "    Two Random Projection techniques: Sparse RP and Gaussian Rp\n",
    "    n_components = final reduced dimension\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(42)\n",
    "    transformer = SparseRandomProjection(n_components=n_components,random_state=rng)\n",
    "    #transformer = GaussianRandomProjection(n_components=n_components,random_state=rng)\n",
    "    train_new = transformer.fit_transform(train)\n",
    "    train_norm_RP= pd.DataFrame(train_new,index=train.index)\n",
    "    test_new = transformer.transform(test)\n",
    "    test_norm_RP= pd.DataFrame(test_new,index=test.index)\n",
    "    testy= yData(test_norm_RP, dataNo)\n",
    "    trainy=yData(train_norm_RP, dataNo)\n",
    "    return np.expand_dims(np.array(train_norm_RP), axis=2), trainy, np.expand_dims(np.array(test_norm_RP), axis=2), testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHkQRaFMYKsi"
   },
   "outputs": [],
   "source": [
    "def Normalization(test, train, dataNo):\n",
    "    if dataNo==1:\n",
    "        train_n=train.iloc[:,4:25337]\n",
    "        test_n=test.iloc[:,4:25337]\n",
    "    else:\n",
    "        train_n=train\n",
    "        test_n=test\n",
    "    \"\"\"\n",
    "    Performs Min-Max Normalization\n",
    "    fit_transform to train data\n",
    "    use statistic from train data to normalize the test data\n",
    "    \"\"\"\n",
    "    x = train_n.values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    train_norm = pd.DataFrame(x_scaled,columns = train_n.columns, index=train_n.index)\n",
    "    x_test = test_n.values\n",
    "    normalized_test_X = min_max_scaler.transform(x_test)\n",
    "    test_norm = pd.DataFrame(normalized_test_X,columns = test_n.columns, index=test_n.index)\n",
    "    print(\"Normalized train data\",train_norm)\n",
    "    print(\"Normalized test data\",test_norm)\n",
    "    return train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1_P_sCHh6hA"
   },
   "outputs": [],
   "source": [
    "def hcluster(trainH, testH):\n",
    "    #Hierarchical clustering with average linkage\n",
    "    sys.setrecursionlimit(30000)\n",
    "    g_train = sns.clustermap(trainH.T,row_cluster=True,method=\"average\")   \n",
    "    #get sorted list of genes\n",
    "    reordered_labels = trainH.T.index[g_train.dendrogram_row.reordered_ind].tolist()\n",
    "    #reorder the test data based on clustering result\n",
    "    test_out=testH.T.reindex(reordered_labels)\n",
    "    test_norm_cluster=test_out.T\n",
    "    #reorder the train data based on clustering result\n",
    "    train_out=trainH.T.reindex(reordered_labels)\n",
    "    train_norm_cluster=train_out.T\n",
    "\n",
    "    return train_norm_cluster,test_norm_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9G9MqadxvLT"
   },
   "outputs": [],
   "source": [
    "train_norm, test_norm = Normalization(test, train, dataNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyyD1b1iK67l"
   },
   "outputs": [],
   "source": [
    "Train, Test=hcluster(train_norm, test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWMO-GVex46u"
   },
   "outputs": [],
   "source": [
    "#Divide into 5 folds\n",
    "if dataNo==1:\n",
    "    train=train.iloc[:,4:25337]\n",
    "train_folds=train.sample(frac=1)\n",
    "length1 = int(len(train_folds)/5) #length of each fold\n",
    "folds1 = []\n",
    "for i in range(4):\n",
    "    folds1 += [train_folds[i*length1:(i+1)*length1]]\n",
    "folds1 += [train_folds[4*length1:len(train_folds)]]\n",
    "folds=folds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG6Cgby0uiOU"
   },
   "source": [
    "# Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSfiw9lryx2r"
   },
   "outputs": [],
   "source": [
    "f = open(\"model_summary.txt\", \"w\")\n",
    "from sklearn import model_selection\n",
    "from keras_tuner.engine import trial\n",
    "class CVTuner(keras_tuner.engine.tuner.Tuner):\n",
    "    \"\"\"\n",
    "    5-fold CV for hyperparameter tuning \n",
    "    \"\"\"\n",
    "    def run_trial(self, trial, *fit_args, **fit_kwargs):\n",
    "        accuracy = []\n",
    "        for i in range(5):\n",
    "            if i==0:\n",
    "                x_train=pd.concat([folds[0], folds[1],folds[2], folds[3]])\n",
    "                x_test=folds[4]\n",
    "            elif i==1: \n",
    "                x_train=pd.concat([folds[0], folds[1],folds[2], folds[4]])\n",
    "                x_test=folds[3]\n",
    "            elif i==2: \n",
    "                x_train=pd.concat([folds[0], folds[1],folds[3], folds[4]])\n",
    "                x_test=folds[2]\n",
    "            elif i==3: \n",
    "                x_train=pd.concat([folds[0], folds[2],folds[3], folds[4]])\n",
    "                x_test=folds[1]\n",
    "            elif i==4: \n",
    "                x_train=pd.concat([folds[1], folds[2],folds[3], folds[4]])\n",
    "                x_test=folds[0]\n",
    "            \"\"\"\n",
    "            The x_train and x_test is train and validation data for one iteration of the CV \n",
    "            Both train and validation data is normalized, clustered in each CV iteration\n",
    "            \"\"\"\n",
    "            train_normIn, test_normIn = Normalization(x_test, x_train, dataNo)\n",
    "            xTrain, xTest=hcluster(train_normIn,test_normIn)\n",
    "            RPdimensions=trial.hyperparameters.Choice('RP_dimension',values=[2500, 3025, 3600,4000, 4225, 4900, 5625, 6400])\n",
    "            x_train_CV, y_train_CV, x_val, y_val = RandomProjection(xTest, xTrain, dataNo, RPdimensions)\n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            print(str(trial.hyperparameters.values))\n",
    "            hist = model.fit(x_train_CV,y_train_CV,\n",
    "            validation_data=(x_val,y_val),\n",
    "            epochs=trial.hyperparameters.Int('epochs', 5, 100, step=5),\n",
    "            batch_size=trial.hyperparameters.Int('batch_size', 32, 128, step=32),)\n",
    "            accuracy.append([hist.history[k][-1] for k in hist.history]) #Accuracy of model for each CV iteration is saved \n",
    "        val_accuracy = np.asarray(accuracy) \n",
    "        f = open(\"model_summary.txt\", \"a\")\n",
    "        f.write(\"acc: \"+str(val_accuracy))\n",
    "        f.write(str(np.mean(val_accuracy[:,1]))) #The accuracy of the model is estimated to be average of accuracy values in 5 iteration\n",
    "        f.close()\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_accuracy[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        tf.keras.models.save_model(model,trial.trial_id)\n",
    "    \n",
    "    \n",
    "def create_model1(hp):\n",
    "    \"\"\"\n",
    "    It is a function to create a 1D CNN model\n",
    "    \"\"\"\n",
    "    if hp:\n",
    "        #Define hyperparameters\n",
    "        dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step= 0.1)\n",
    "        output = hp.Float('output', min_value=50, max_value=300, step= 50)\n",
    "        filters=hp.Choice('filters',values=[8, 16, 32, 64, 128, 256])\n",
    "        kernel_size=hp.Choice('kernel_size', values = [2,3,5,7,11])\n",
    "        #learning_rate = hp.Float('learning_rate', min_value=0.00001, max_value=0.1, step=0.00001)\n",
    "        learning_rate=1e-4\n",
    "        num_hidden_layers = hp.Choice('num_hidden_layers', values=[1, 2, 3])\n",
    "        RP_dimension=hp.Choice('RP_dimension',values=[2500, 3025, 3600,4000, 4225, 4900, 5625, 6400])\n",
    "    else:\n",
    "        dropout_rate = 0.1\n",
    "        num_units = 8\n",
    "        learning_rate = 1e-4\n",
    "        num_hidden_layers = 1\n",
    "        filters=64\n",
    "        kernel_size=11\n",
    "        RP_d=3600\n",
    "\n",
    "    x_train_CV, y_train_CV, x_val, y_val = RandomProjection(Test, Train, dataNo, RP_dimension)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Conv1D(filters,kernel_size, activation='relu', input_shape=(x_train_CV.shape[1], x_train_CV.shape[2])))\n",
    "    if dataNo==1:\n",
    "        classNo = 3\n",
    "    elif dataNo==2:\n",
    "        classNo=4\n",
    "    else: \n",
    "        classNo=2\n",
    "\n",
    "\n",
    "  \n",
    "    for _ in range(0, num_hidden_layers):\n",
    "        model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(output, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(classNo, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      #optimizer=tf.keras.optimizers.Adam(tf.cast(learning_rate, tf.float32)),\n",
    "      optimizer=tf.keras.optimizers.SGD(hp.Choice('learning_rate',values=[1e-2,0.005, 1e-3, 0.0005, 1e-4]),nesterov=True),\n",
    "      metrics=['accuracy']\n",
    "    )   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUzMznarMfZJ"
   },
   "outputs": [],
   "source": [
    "#Select the tuner class (Bayesian, Hyperband, or Random Search), the hypermodel and objective to optimize\n",
    "tuner = CVTuner(\n",
    "    hypermodel=create_model1,\n",
    "    oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "    objective=keras_tuner.Objective(\"val_accuracy\", \"max\"), max_trials=50),\n",
    "    directory='.',\n",
    "    project_name='my_projectSingle',\n",
    "    #overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHNr7erKMq_7"
   },
   "outputs": [],
   "source": [
    "#Start the tuner search\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5) \n",
    "tuner.search(Train, trainY, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=10)[0]\n",
    "RP_d= best_hps.get('RP_dimension')\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number Random Projection dimension is {best_hps.get('RP_dimension')}.\n",
    "\"\"\")\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XabhWP_vMu9X"
   },
   "outputs": [],
   "source": [
    "#Randomly project with an optimal RP the global Train and Test data <- already normalized and hierarchically clustered \n",
    "x_train, y_train, x_test, y_test = RandomProjection(Test, Train, dataNo, RP_d)\n",
    "#Build the model with best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "#Fit the model with the dataset\n",
    "model.fit(x_train, y_train, epochs=best_hps.get('epochs'), validation_split=0.2)\n",
    "#Evaluate the model\n",
    "eval_result = model.evaluate(x_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSkpf9DbrORD"
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFVJNIkj1z2q"
   },
   "outputs": [],
   "source": [
    "#Select the tuner class (Bayesian, Hyperband, or Random Search), the hypermodel and objective to optimize\n",
    "tuner = CVTuner(\n",
    "    hypermodel=create_model1,\n",
    "    oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "    objective=keras_tuner.Objective(\"val_accuracy\", \"max\"), max_trials=50),\n",
    "    directory='.',\n",
    "    project_name='my_projectEnsemble',\n",
    "    #overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37QQA4EMrnZ3"
   },
   "outputs": [],
   "source": [
    "def hardVoting(y_classList, y_test):\n",
    "  \"\"\" sums the predictions for each class label and predict the class label with the majority votes\"\"\"\n",
    "  acc=0\n",
    "  print('Y_classlist:', len(y_classList))\n",
    "  print('Y_test:', len(y_test))\n",
    "  y_hat=[None] * (len(y_test))\n",
    "  for i in range(len(y_test)):\n",
    "    count0=0\n",
    "    count1=0\n",
    "    count2=0\n",
    "    count3=0\n",
    "    for ii in range(len(y_classList)):\n",
    "      print(y_classList[ii][i])\n",
    "      if(y_classList[ii][i]==0):\n",
    "        count0=count0+1\n",
    "      if(y_classList[ii][i]==1):\n",
    "        count1=count1+1\n",
    "      if(y_classList[ii][i]==2):\n",
    "        count2=count2+1\n",
    "      if(y_classList[ii][i]==3):\n",
    "        count3=count3+1\n",
    "    classMax=max(count0, count1, count2, count3)\n",
    "    if classMax==count0:\n",
    "      y_hat[i]=0\n",
    "    if classMax==count1:\n",
    "      y_hat[i]=1\n",
    "    if classMax==count2:\n",
    "      y_hat[i]=2\n",
    "    if classMax==count3:\n",
    "      y_hat[i]=3\n",
    "    if(y_hat[i]==y_test[i]):\n",
    "      acc=acc+1\n",
    "    print('class:',y_hat[i])\n",
    "    #if(y_classList[1][i]==testY[i]):\n",
    "    #  accS=accS+1\n",
    "  accOut=acc/(len(y_test))\n",
    "  print('Hard Voting acuracy: ',accOut)\n",
    "  return accOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRwDllu0rwb_"
   },
   "outputs": [],
   "source": [
    "def softVoting(y_predictList, y_test):\n",
    "  \"\"\"sums the predicted probabilities for each class label and predic the class label with the largest probability\"\"\"\n",
    "  y_out=[0]*(len(y_test))\n",
    "  Y_out=[0]*(len(y_test))\n",
    "  acc=0\n",
    "  for i in range(len(y_test)):\n",
    "    for ii in range(len(y_predictList)):\n",
    "      y_out[i]= y_out[i]+y_predictList[ii][i]\n",
    "    max_value = max(y_out[i])\n",
    "    listy=y_out[i].tolist()\n",
    "    max_index = listy.index(max_value)\n",
    "    Y_out[i]=max_index\n",
    "    if(Y_out[i]==y_test[i]):\n",
    "      acc=acc+1\n",
    "  print(\"predicted class\", Y_out)\n",
    "  accOut=acc/(len(y_test))\n",
    "  print('Soft Voting acuracy: ',accOut)\n",
    "  return accOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dk-UhPlOrzLG"
   },
   "outputs": [],
   "source": [
    "f = open(\"model_summary.txt\", \"w\")\n",
    "L=[5,10,20,30,40,50,60,70,80,90,100] #number of base classifiers\n",
    "#L=[30,35,40,45,50]\n",
    "resultsMajV =[]\n",
    "resultsSoft=[]\n",
    "for i in L:\n",
    "  l=i\n",
    "  y_predictList=[]\n",
    "  y_classList=[]\n",
    "  for ii in range(l):\n",
    "    # train l number of base classifiers\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5) \n",
    "    tuner.search(Train, trainY, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=10)[0]\n",
    "    RP_d= best_hps.get('RP_dimension')\n",
    "    x_train, y_train, x_test, y_test = RandomProjection(Test, Train, dataNo, RP_d)\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    model.fit(x_train, y_train, epochs=best_hps.get('epochs'), validation_split=0.2)\n",
    "    Y= model.predict(x_test)\n",
    "    y_classes = Y.argmax(axis=-1)\n",
    "    y_predictList.append(Y)       # Save class probability values for each base classifier\n",
    "    y_classList.append(y_classes) #Save class labels for each base classifier\n",
    "    if l==len(y_predictList):     #Calls softVoting and hardVoting functions   \n",
    "      accMajVote= hardVoting(y_classList, y_test)\n",
    "      accSoft= softVoting(y_predictList, y_test)\n",
    "      resultsMajV.append(accMajVote)\n",
    "      resultsSoft.append(accSoft) \n",
    "      #Saves the results for majority and soft voting in a file\n",
    "      f = open(\"model_summary.txt\", \"a\")\n",
    "      f.write(\"\\n For l dimensions:\"+ str(l))\n",
    "      f.write(\"\\n SoftVoting acc: \"+str(accSoft))\n",
    "      f.write(\"\\nHard acc: \"+str(accMajVote))\n",
    "      f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNNwithCLUSTER_final (3) (2) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
